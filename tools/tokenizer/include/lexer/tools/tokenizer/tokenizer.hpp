#ifndef LEXER_TOOLS_TOKENIZER_INCLUDE_LEXER_TOOLS_TOKENIZER_TOKENIZER_HPP
#define LEXER_TOOLS_TOKENIZER_INCLUDE_LEXER_TOOLS_TOKENIZER_TOKENIZER_HPP

#include <expected>
#include <optional>
#include <string>
#include <string_view>

#include "lexer/core/lexer.hpp"
#include "lexer/tools/tokenizer/error.hpp"
#include "lexer/tools/tokenizer/token.hpp"

namespace lexer::tools::tokenizer
{
/**
 * @brief Wrapper that turns core::Lexer into a sequential token stream.
 *
 * Returns tokens in order as matched by the lexer without additional processing.
 */
class Tokenizer
{
public:
    /**
     * @brief Standard tokenizer result type.
     *
     * Holds a `Token<T>` on success, `std::nullopt` on end of input, or an `Error` on failure.
     */
    template <typename T>
    using Result_t = std::expected<std::optional<Token<T>>, Error>;

    /**
     * @brief Construct a tokenizer from a lexer.
     * @param lexer Lexer used to recognize tokens.
     */
    explicit Tokenizer(core::Lexer lexer) : lexer_{std::move(lexer)}, offset_{0} {}

    /**
     * @brief Construct a tokenizer from a lexer and an input string held in memory.
     * @param lexer Lexer used to recognize tokens.
     * @param input Input text to tokenize.
     */
    explicit Tokenizer(core::Lexer lexer, std::string input)
        : lexer_{std::move(lexer)}, input_{std::move(input)}, offset_{0}
    {}

    /**
     * @brief Replace the input text and reset tokenization state.
     */
    void load(std::string input)
    {
        input_ = std::move(input);

        offset_ = 0;
    }

    /**
     * @brief Reset the reading position to the beginning of the current input.
     */
    void reset() noexcept { offset_ = 0; }

    /**
     * @brief Return the next token.
     *
     * On success, returns a Token<T> wrapped in std::optional where std::nullopt indicates end of input.
     * On failure, returns an Error describing the lexical error at the current position.
     */
    template <typename T>
        requires(std::integral<T> || std::is_enum_v<T>)
    [[nodiscard]] Result_t<T> next()
    {
        if (offset_ >= input_.size())
        {
            return std::nullopt;
        }

        const auto view{std::string_view{input_}.substr(offset_)};

        const auto [token, consumed]{lexer_.tokenize<T>(view)};

        if (!token || consumed == 0)
        {
            return std::unexpected(Error{"Unrecognized character at position " + std::to_string(offset_), offset_});
        }

        const auto lexeme{std::string_view{input_}.substr(offset_, consumed)};

        offset_ += consumed;

        return Token<T>{*token, lexeme};
    }

private:
    core::Lexer lexer_;

    std::string input_;

    std::size_t offset_;
};

} // namespace lexer::tools::tokenizer

#endif // LEXER_TOOLS_TOKENIZER_INCLUDE_LEXER_TOOLS_TOKENIZER_TOKENIZER_HPP
